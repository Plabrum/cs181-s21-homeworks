\documentclass[submit]{harvardml}

\course{CS181-S21}
\assignment{Assignment \#2}
\duedate{7:59pm EST, Feb 19th, 2021}

\usepackage[OT1]{fontenc}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{subfig}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{framed}
\usepackage{color}
\usepackage{soul}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{common}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{float}
\newcommand{\B}{\text{B}}
\newcommand{\Beta}{\text{Beta}}

\usepackage[mmddyyyy,hhmmss]{datetime}

\definecolor{verbgray}{gray}{0.9}

\lstnewenvironment{csv}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  columns=fullflexible}}{}

\begin{document}

\begin{center}
{\Large Homework 2: Classification and Bias-Variance Trade-offs}\\
\end{center}

\subsection*{Introduction}

This homework is about classification and bias-variance trade-offs. In
lecture we have primarily focused on binary classifiers trained to
discriminate between two classes. In multiclass classification, we
discriminate between three or more classes.  Most of the material for Problem 1 and Problem 3, and all of the material for Problem 2 will be covered by the end of the Tuesday 2/9 lecture. The rest of the material will be covered by the end of the Thursday 2/11 lecture.  We encourage you to read
CS181 Textbook's Chapter 3 for more information on linear
classification, gradient descent, classification in the discriminative
setting (covers multiclass logistic regression and softmax), and
classification in the generative setting. Read Chapter 2.8 for more
information on the trade-offs between bias and variance.

As a general note, for classification problems we imagine that we have
the input matrix $\boldX \in \reals^{N \times D}$ (or perhaps they
have been mapped to some basis $\bm{\Phi}$, without loss of
generality) with outputs now ``one-hot encoded."  This means that if
there are~$K$ output classes, rather than representing the output
label $y$ as an integer~${1,2,\ldots,K}$, we represent $\boldy$ as a
``one-hot" vector of length~$K$. A ``one-hot" vector is defined as
having every component equal to 0 except for a single component which
has value equal to 1.  For example, if there are $K = 7$ classes and a
particular data point belongs to class 3, then the target vector for
this data point would be~$\boldy = [0,0,1,0,0,0,0]$.  We will define
$C_1$ to be the one-hot vector for the 1st class, $C_2$ for the 2nd
class, etc.  Thus, in the previous example $\boldy = C_3$. If there
are $K$ total classes, then the set of possible labels is $\{C_1
\ldots C_K \} = \{C_k\}_{k=1}^K$.  Throughout the assignment we will
assume that each label $\boldy \in \{C_k\}_{k=1}^K$ unless otherwise
specified. The most common exception is the case of binary classification
($K = 2$), in which case labels are the typical integers $y \in \{0, 1\}$.\\

In problems 1 and 3, you may use \texttt{numpy} or \texttt{scipy}, but
not \texttt{scipy.optimize} or \texttt{sklearn}. Example code given is
in Python 3.\\

Please type your solutions after the corresponding problems using this
\LaTeX\ template, and start each problem on a new page.\\

Please submit the \textbf{writeup PDF to the Gradescope assignment `HW2'}. Remember to assign pages for each question.  \textbf{You must include your plots in your writeup PDF. } The supplemental files will only be checked in special cases, e.g. honor code issues, etc. \\

Please submit your \textbf{\LaTeX\ file and code files to the Gradescope assignment `HW2 - Supplemental'}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[Exploring Bias and Variance, 10 pts]
  In this problem, we will explore the bias and variance of a
  few different model classes when it comes to logistic regression.

  Consider the true data generating process $y \sim \text{Bern}(f(x)), f(x) = \sigma(\sin x)$, where $\sigma(z)$ is the sigmoid function
  $\sigma(z)= (1+\exp[-z])^{-1}$, $x \in \mathbb{R}$, and $y \in \{0,1\}$.
  Recall that for a given $x$, bias and variance are defined in terms of expectations \textit{over randomly drawn datasets} $D$
  from this underlying data distribution:
  \begin{align*}
  \text{Bias}[\hat{f}(x)] &= \mathbb{E}_D[\hat{f}(x)] - f(x)\\
  \text{Variance}[\hat{f}(x)] &= \mathbb{E}_D[(\hat{f}(x) - \mathbb{E}_D[\hat{f}(x)])^2]
  \end{align*}
  Here, $\hat{f}(x)$ is our estimator (learned through logistic regression on a given dataset $D$).
  We will directly explore the bias-variance trade-off by drawing multiple such datasets and fitting different logistic regression models to each.
  Remember that we, the modelers, do not usually see the true data distribution.
  Knowledge of the true $f(x)$ is only exposed in this problem to 1) make possible the simulation
  of drawing multiple datasets, and 2) to serve as a pedagogical tool in allowing
  verification of the true bias.

\begin{enumerate}

\item Consider the three bases $\phi_1(x) = [1, x]$,
  $\phi_2(x) = [1, x, x^2, x^3]$, $\phi_3(x) = [1, x, x^2, x^3, x^4, x^5]$.
  For each of these bases, generate 10 datasets of size $N = 10$ using the starter code provided, and fit a logistic regression model using sigmoid($w^T \phi(x)$) to each dataset by using
  gradient descent to minimize the negative log likelihood. Note that the classes are represented with 0's and 1's.
  This means you will be running gradient descent 10 times for each basis, once for each dataset.

  Use random starting values of $w$, $\eta=0.001$, take 10,000 update steps
   for each gradient descent run, and make sure to average the gradient over the data points
   (for each step). These parameters, while not perfect, will ensure your code
   runs in a reasonable amount of time. The emphasis of this problem is on
   capturing the bias-variance trade-off, so don't worry about attaining perfect precision in the gradient descent
   as long as this trade-off is captured in the final models.

   Note: Overflow RuntimeWarnings due to np.exp should be safe to ignore, if any.

\item Create three plots, one for each basis. Starter code is available which you may modify.
By default, each plot displays three types of functions:
1) the true data-generating distribution,
2) all 10 of the prediction functions learned from each randomly drawn dataset, and
3) the mean of the 10 prediction functions.
Moreover, each plot also displays 1 of the randomly generated datasets and highlights the corresponding prediction function learned by this dataset.

\item Explain what you see in terms of the bias-variance trade-off.
How do the fits of the individual and mean prediction functions change?
Keeping in mind that none of the model classes match the true generating process exactly, discuss the extent to which each of the bases approximates the true process.

\item If we were to increase the size of each dataset drawn from $N = 10$ to a larger number, how would the variance change? Why might this be the case?

\end{enumerate}

\end{problem}

\newpage

\subsection*{Solution 2}
\begin{enumerate}
    \item Generate 10 datasets of size N and fit the regression model for each basis.
    \\
    The Python code to generate and fit is submitted in T2\_P1.py
    \\
    \item Create plots for each basis:
    \begin{enumerate}
        \item Using Basis 1: $\phi_1(x) = [1, x]$
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw2/P1_plots/Basis 1 Plot.png}
            \centering
        \end{figure}
        \item Using Basis 2: $\phi_2(x) = [1, x, x^2, x^3]$
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw2/P1_plots/Basis 2 Plot.png}
            \centering
        \end{figure}
        \item Using Basis 3: $\phi_3(x) = [1, x, x^2, x^3, x^4, x^5]$
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw2/P1_plots/Basis 3 Plot.png}
            \centering
        \end{figure}
    \end{enumerate}
    
    \item We can understand the bias-variance trade-off here by looking at the various lines in the graphs:\\
    Using the definitions:
    \begin{align*}
    \text{Bias}[\hat{f}(x)] &= \mathbb{E}_D[\hat{f}(x)] - f(x)\\
    \text{Variance}[\hat{f}(x)] &= \mathbb{E}_D[(\hat{f}(x) - \mathbb{E}_D[\hat{f}(x)])^2]
    \end{align*}
    We can see that $\mathbb{E}_D[\hat{f}(x)]$ is represented by the "Mean of learned models" - the black line, while $f(x)\textbf{}$ is represented using the "Ground truth model" - the green line. Therefore, we can visually see the bias as the difference between the black line and the green line. \\
    In a similar sense, we can see the variance as the difference between  the thin lines each representing $\hat{f}(x)$ and the average $\hat{f}(x)$ black line.\\
    
    Using this information we can visualize the bias variance trade-off across each of these plots.\\
    \textbf{Plot 1} has a low variance, we can see this as all of the $\hat{f}(x)$ models are quite close to the mean, however. this has an effect on the bias as the average of the models does not fit the ground truth model very well.\\
    \textbf{Plot 2} This plot has a larger variance but a reduced bias (although not by much) this is shown by the $\hat{f}(x)$ fitting the dataset more closely and moving farther from the mean line.\\
    Finally, \textbf{Plot 3} has a very high variance, but by a closer approximation of the underlying data - the black line moves a little in relation to the underlying sine wave, however, you can see the over fitting beginning with the sharp difference and steps on the mean line. I think basis 2 works best for this use case, the individual lines are similar approximations for the underlying dataset without over fitting too much.
    
    \item When we increase the size of each dataset the bias-variance trade-off is accentuated, we would see the first basis forming essentially straight lines with a high bias and the last having a small bias but a very high variance. This is the case because the model will have so much data on which to fit itself, by becoming very tailored to the dataset there is ample opportunity to over fit and lose track of the underlying model.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[Maximum likelihood in classification, 15pts]

  Consider now a generative $K$-class model.  We adopt class prior
  $p(\boldy = C_k; \bpi) = \pi_k$ for all $k \in \{1, \ldots, K\}$
(where $\pi_k$ is a parameter of the prior).
Let  $p(\boldx|\boldy=C_k)$ denote
the class-conditional density of features $\boldx$ (in this
case for class $C_k$). Consider the data set $D = \{(\boldx_i,
\boldy_i)\}_{i=1}^n$ where as above $\boldy_i \in \{C_k\}_{k=1}^K$ is
encoded as a one-hot target vector and the data are independent.

\begin{enumerate}
  \item Write out the negative log-likelihood of the data set, $-\ln p(D ; \bpi)$.

  \item Since the prior forms a distribution, it has the constraint that
    $\sum_k\pi_k - 1 = 0$.  Using the hint on
Lagrange multipliers below, give the
    expression for the maximum-likelihood estimator for the prior
    class-membership probabilities, i.e.
    $\hat \pi_k.$
    Make sure to write out the intermediary equation you need
    to solve to obtain this estimator. Briefly state why your final answer is intuitive.
\end{enumerate}

    For the remaining questions, let the
    class-conditional probabilities be Gaussian distributions with
the same covariance matrix
    $$p(\boldx | \boldy = C_k) = \mathcal{N}(\boldx |  \bmu_k, \bSigma), \text{\ for\ }k \in \{1,\ldots, K\}$$
    and different means $\bmu_k$ for each class.

    \begin{enumerate}
  \item[3.] Derive the gradient of the negative log-likelihood with respect to vector $\bmu_k$.
    Write the expression in matrix form as a function of the variables defined
    throughout this exercise. Simplify as much as possible for full credit.
  \item[4.] Derive the maximum-likelihood estimator $\hat{\mu}_k$ for vector $\bmu_k$. Briefly state why your final answer is intuitive.
  \item[5.] Derive the gradient for the negative log-likelihood with respect to the
    covariance matrix $\bSigma$ (i.e., looking
to find an MLE for the covariance).
Since you are differentiating with respect to a
    \emph{matrix}, the resulting expression should be a matrix!
%
  \item[6.] Derive the maximum likelihood estimator $\hat{\Sigma}$ of the covariance matrix.
\end{enumerate}

\paragraph{Hint: Lagrange Multipliers.} Lagrange Multipliers are a method for
optimizing a function $f$ with respect to an
equality constraint, i.e.
\[\min_{\boldx} f(\boldx)\ \text{s.t.}\ g(\boldx) = 0.\]

This can be turned into an unconstrained problem by introducing a
Lagrange multiplier $\lambda$ and constructing the Lagrangian function,
\[L(\boldx, \lambda) =  f(\boldx) + \lambda g(\boldx).\]

It can be shown that it is a necessary condition that the optimum
is a critical point of this new function. We can find this point by solving two equations:

\[\frac{\partial L(\boldx, \lambda)}{\partial  \boldx} = 0  \ \ \text{and}\  \  \frac{\partial L(\boldx, \lambda)}{\partial \lambda} = 0 \]


\paragraph{Cookbook formulas.} Here are some formulas you might want to consider
using to compute difficult gradients. You can use them  in the homework
without proof. If you are looking to hone your matrix calculus skills, try to
find different ways to prove these formulas yourself (will not be part of the
evaluation of this homework). In general, you can use any formula from the matrix cookbook,
as long as you cite it. We opt for the following common notation:
$\boldX^{-\top} := (\boldX^{\top})^{-1}$
\begin{align*}
  & \frac{\partial \bolda^\top \boldX^{-1} \boldb}{\partial \boldX} = - \boldX^{-\top} \bolda \boldb^\top \boldX^{-\top} \\
  & \frac{\partial \ln | \det (\boldX) |}{\partial \boldX} = \boldX^{-\top}
 \end{align*}
 \end{problem}


\subsection*{Solution 2}
\textbf{Part 1:} The negative log likelihood of the data set: $-\ln p(D; \pi)$

$$
 p(D;\pi) = \prod_{i=1}^{N} p(x_i, y_i; \pi)
$$
$$
= \prod_{i=1}^{N} p(x_i| y_i, \pi) p(y_i|\pi) \\
$$
$$
= \prod_{i=1}^{N} \prod_{k=1}^{K} (p(x_i| y_i = C_k) \pi_k)^{y_{ik}} \\
$$
$$
-\ln p(D; \pi) = - \sum_{i=1}^N \sum_{k=1}^K y_{ik}[ln(p(x_i|y_i = C_k) \pi_k)]
$$
\\

\textbf{Part 2:} The expression for the maximum-likelihood estimator: $\hat{\pi_k}$

We can use the constraint $\sum_k\pi_k - 1 = 0$ and form a Larange Multiplier:
$$
    L(\pi, \lambda) = -\ln p(D; \pi) + \lambda(\sum^k (\pi_k) -1)
$$
With this expression we can take the derivative of $L(\pi, \lambda)$ (using log rules to split the expression):\\
$$
    \frac{\partial L(\pi_k, \lambda)}{\partial \pi_k} = \frac{\partial}{\partial \pi_k} \left[ \sum^N_i \sum^K_k y_{ik} \left( \ln (p(x_i | y_i = C_k )) + \ln(\pi_k)  \right) - \lambda(\sum^k (\pi_k) -\lambda  \right]
$$
As we are taking the derivative with respect to $\pi_k$ we can simplify:

$$
    \frac{\partial L(\pi_k, \lambda)}{\partial \pi_k} =
    \sum^N_i \sum^K_k \frac{y_{ik}}{\pi_k} - \lambda\sum^k 1
$$

$$
    \frac{\partial L(\pi_k, \lambda)}{\partial \pi_k} =
    \sum^N_i \sum^K_k \frac{y_{ik}}{\pi_k} - \lambda k
$$
Now we can simplify for a given class and set the derivative to 0:
$$
    0 = \sum^N_i  \frac{y_{ik}}{\pi_k} - \lambda k
$$
We can define $N_k$ as the number of elements in the class k:
$$
    \pi_k = \frac{N_k} {\lambda}
$$
Now we can take the derivative with respect to $\lambda$:
$$
frac{\partial L(\pi_k, \lambda)}{\partial \lambda} = \sum^k (\pi_k) -1
$$
And set it = 0:
$$
\sum^k (\pi_k) = 1
$$
Finally we can substitute in our value of $\pi_k$ from before:
$$
\sum^k (\frac{N_k} {\lambda}) = 1
$$
We can see that $\lambda=\sum^k N_k$, therefore $\lambda$ is equal to the number of values in out dataset as we have sum across each class. \\
Therefore, $\pi_k = \frac{N_k}{\lambda}$\\

This answer is intuitive because the probability of being in class $k$ is simply the fraction of values in class $k$ to all values.

\item \textbf{Part 3} Derive the gradient of the MLE with respect to vector $\mu_k$:
$$
    L(\pi, \lambda) = -\ln p(D; \pi) + \lambda(\sum^k (\pi_k) -1)
$$

As we can assume:
$$p(\boldx | \boldy = C_k) = \mathcal{N}(\boldx |  \bmu_k, \bSigma), \text{\ for\ }k \in \{1,\ldots, K\}$$
We can substitute the following into our dervative:
$$
    \frac{\partial L(\pi_k, \lambda)}{\partial \mu_k} = \frac{\partial}{\partial \pi_k} \left[ \sum^N_i \sum^K_k y_{ik} 
    \left( \ln ( \mathcal{N}(\boldx |  \bmu_k, \bSigma) + \ln(\pi_k) \right) \right]
$$
We can now substitute the multivariate normal distribution into our equation:
$$
    \frac{\partial L(\pi_k, \lambda)}{\partial \mu_k} = \frac{\partial}{\partial \pi_k} \left[ \sum^N_i \sum^K_k y_{ik}
    \left( \ln 
    \left(\frac{1}{\sqrt{(2\pi)^n|\boldsymbol\Sigma|}}
    \exp\left(-\frac{1}{2}({\boldsymbol x_i}-{\boldsymbol \mu_k})^T {\boldsymbol\Sigma}^{-1}({\boldsymbol x_i}-{\boldsymbol \mu_k})
    \right)\right)
    + \ln(\pi_k) \right) \right]
$$
Massively simplifying and using cookbook 84:
$$
    \frac{\partial L(\pi_k, \lambda)}{\partial \mu_k} = \left[ \sum^N_i \sum^K_k y_{ik} ( \Sigma^{-1} (\mu_k -x_i))  \right]
$$

\item \textbf{Part 4} Derive the MLE estimator\\
Continuing from before, we can set our gradient to 0:
$$
    \left[ \sum^N_i \sum^K_k y_{ik} ( \Sigma^{-1} (\mu_k -x_i))  \right] = 0
$$
For a given class (and therefore a given $\mu_k$) we can simplify and remove the covariance matrix from the summation as it is fixed after fitting.\\
$$
    \at{\mu_k} = \frac{\Sigma^-1 \sum^n_i y_{ik}x_i}{\Sigma^-1 \sum^n_i y_{ik}} 
$$
$$
    \at{\mu_k} = \frac{ \sum^n_i y_{ik}x_i}{\sum^n_i y_{ik}} 
$$
\item \textbf{Part 5} Derive the gradient to the negative log-likelihood operator with respect to the covariance matrix.\\
As the covariance matrix is symmetric, we can use the matrix cookbook formula:
\begin{align*}
  & \frac{\partial \bolda^\top \boldX^{-1} \boldb}{\partial \boldX} = - \boldX^{-\top} \bolda \boldb^\top \boldX^{-\top} \\
  & \frac{\partial \ln | \det (\boldX) |}{\partial \boldX} = \boldX^{-\top}
 \end{align*}
 
$$
    \frac{\partial L(\pi_k, \lambda)}{\partial \Sigma} = \frac{1}{2} \left[ \sum^N_i \sum^K_k y_{ik} ( \Sigma^{-T} - \Sigma^{-T}(x_i - \mu_k)(\mu_k - x_i)^{T}\Sigma^{-T}   \right]
$$

\item \textbf{Part 6} Derive the  Maximum likelihood estimator $\hat{\Sigma}$:
We take the above gradient and set it to 0 as before:\\
$$
    \frac{1}{2} \left[ \sum^N_i \sum^K_k y_{ik} ( \Sigma^{-T} - \Sigma^{-T}(x_i - \mu_k)(\mu_k - x_i)^{T}\Sigma^{-T}   \right] = 0
$$
Therefore we are left with:
$$
    \hat{\Sigma} = \frac{\sum^N_i \sum^K_k y_{ik} (x_i - \mu_k)(\mu_k - x_i)^{T}}{\sum^N_i \sum^K_k y_{ik}}
$$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}[Classifying Stars, 15pts]

You're tasked with classifying three different kinds of stars using their magnitudes and temperatures. See star.png for a plot of
the data, adapted from
\url{http://astrosci.scimuze.com/stellar_data.htm} and available as
\verb|data/hr.csv|, which you will find in the Github repository. \\

The CSV file has three columns: type, magnitude, and temperature. The
first few lines look like this:
\begin{csv}
Type,Magnitude,Temperature
Dwarf,-5.8,-0.35
Dwarf,-4.1,-0.31
...
\end{csv}

In this problem, you will code up 4 different classifiers for this task:
\begin{enumerate}[label=\alph*)]

\item \textbf{A three-class generalization of logistic regression}, also
  known as softmax regression, in which you implement gradient descent on the negative log-likelihood. In Question 2 you will explore the effect of using different values for the learning rate $\eta$ (\texttt{self.eta}) and
  regularization strength $\lambda$ (\texttt{self.lam}).  Make sure to include a bias term and to
  use L2 regularization. See CS181 Textbook's Chapter 3.6 for details on multi-class
  logistic regression and softmax.
  
\item \textbf{A generative classifier with Gaussian class-conditional
  densities with a \textit{shared covariance} matrix} across all classes. 
  Feel free to re-use your Problem 2 results.
\item \textbf{Another generative classifier with Gaussian class-conditional densities , but now 
with a \textit{separate covariance} matrix} learned for each class. (Note: 
The staff implementation can switch between the two Gaussian generative classifiers with just a
few lines of code.)

\item \textbf{A kNN classifier} in which you classify based on the $k=1,3,5$ nearest neighbors and the following distance function: $$dist(star_1, star_2) = ((mag_1 - mag_2)/3)^2 + (temp_1 - temp_2)^2$$
where nearest neighbors are those with the smallest distances from a given point.

  Note 1: When there are more than two labels, no label may have the
  majority of neighbors.  Use the label that has the most votes among
  the neighbors as the choice of label. 

  Note 2: The grid of points for which you are making predictions
  should be interpreted as our test space.  Thus, it is not necessary
  to make a test point that happens to be on top of a training point
  ignore itself when selecting neighbors.

\end{enumerate}

After implementing the above classifiers, complete the following exercises:

\begin{enumerate}
    \item Plot the decision boundaries generated by each classifier for the dataset. Include them in your PDF. 
    Identify the similarities and differences among the classifiers. What explains the differences?
    
    \item For logistic regression only,  make a plot with
      ``Number of Iterations" on the x-axis and ``Negative Log-Likelihood Loss" on the y-axis for several
      configurations of the hyperparameters $\eta$ and $\lambda$.  Specifically,  try the values $0.05$,  $0.01$,  and $0.001$ for each hyperparameter.  Limit the number of gradient descent iterations to 200,000.  What are your final choices of learning rate
      ($\eta$) and regularization strength ($\lambda$), and why are they reasonable? How
      does altering these hyperparameters affect the ability to converge,  the rate of convergence,  and the final loss (a qualitative description is sufficient)? You only need to submit one plot for your final choices of hyperparameters.

    \item For both Gaussian generative models, report the negative log-likelihood loss. Which model has a lower loss, and why?
      For the separate covariance model, be sure to use
      the covariance matrix that matches the true class of each data
      point.
    
    \item Consider a star with Magnitude 6 and Temperature 2.
      To what class does each classifier assign this star? Do the
      classifiers give any indication as to whether or not you should
  trust them?
\end{enumerate}
\end{problem}

\newpage

\begin{framed}
\noindent\textbf{Problem 3} (cont.)\\


\textbf{Implementation notes:} Run the controller file, \texttt{T2\_P3.py},
to test your code. Write the actual implementations in the \texttt{GaussianGenerativeModel},
\texttt{LogisticRegression}, and \texttt{KNNModel} classes, which are defined in the three
\texttt{T2\_P3\_ModelName.py} files. These classes follow the same interface pattern
as sklearn. Their code
currently outputs nonsense predictions just to show the
high-level interface, so you should replace their \texttt{predict()} implementations.
You'll also need to modify the hyperparameter
values in \texttt{T2\_P3.py} for logistic regression.
\end{framed}


\subsection*{Solution 3}

\begin{enumerate}
    \item Plot the decision boundaries generated by each dataset:
    \begin{enumerate}
        \item Plot for the Logistic Regression Model
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw2/P3_plots/logistic_regression_result.png}
            \centering
        \end{figure}
        \item Plot for the Generative Gaussian Model with seperate covariance
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw2/P3_plots/generative_result_separate_covariances.png}
            \centering
        \end{figure}
        \item Plot for the Generative Gaussian Model with shared covariance
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw2/P3_plots/generative_result_shared_covariances.png}
            \centering
        \end{figure}
        \item Plot for the KNN Model with K=1
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw2/P3_plots/knn1_result.png}
            \centering
        \end{figure}
        \item Plot for the KNN Model with K=3
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw2/P3_plots/knn3_result.png}
            \centering
        \end{figure}
        \item Plot for the KNN Model with K=5
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw2/P3_plots/knn5_result.png}
            \centering
        \end{figure}
    \end{enumerate}
    \item Log-Likelihood Loss plot For Eta=0.01 Lambda=0.001:
    \begin{figure}[H]
        \includegraphics[width=8cm]{hw2/P3_plots/logistic_regression_loss.png}
        \centering
    \end{figure}
    My choices for eta and lambda are chosen so that the log-likelihood has a smooth descent and the lowest final log-likelihood loss value. The values of 0.01 and 0.001 for eta and lambda respectively make sense as we need a small enough eta to not overshoot the optimum and cause oscillation (as found in the  0.05 case). It also makes sense that the regularization penalty is smaller than the learning rate, it seems that in the case $lambda>eta$ the model does not reach the lowest possible loss value.\\
    Of all of the plots, this combination had the lowest total loss value, and one of the faster convergence rates (but not so fast that it reduces the convergence efficacy).
    \item Report the negative log-likelihood loss for the Gaussian Models:\\
    \textbf{Separate Covariance negative log-likelihood:} 63.97035984092419 \\
    \textbf{Shared Covariance negative log-likelihood:} 116.39446507788162
    \\
    The Separate co-variance model has a lower loss - this result is intuitive as it is able to tailor itself to the data more closely, thereby further minimizing. This can be seen on the graph with the prediction areas closely matching the datapoints.\\
    \item Star with Magnitude 6 and Temperature 2:
    \begin{enumerate}
        \item \textbf{Linear Regression Model: Giant}
        \item \textbf{Shared Gaussian Model: Giant}
        \item \textbf{Separate Gaussian Model: Dwarf}
        \item \textbf{KNN with K=1 Model: Dwarf}
        \item \textbf{KNN with K=3 Model: Dwarf}
        \item \textbf{KNN with K=5 Model: Dwarf}\\\\
        These classifier do not specifically give indication as to how trustworthy the predictions are, but, having said that - I would prioritize selecting models that have smooth decision boundaries around the classes. The K=1 and shared Gaussian models lend appear much more generalize-able to all new data from outside of our recorded dataset. Conversely, I would not trust the predictions of the Gaussian model with separate covariance matrices, the tight decision boundaries seem too fitted to the data. 
    \end{enumerate}
\end{enumerate}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Name and Calibration
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Name}
Phil Labrum
\subsection*{Collaborators and Resources}
Natalie Marguliez
Thomas Maldonado
Daniel Rodriguez

\subsection*{Calibration}
A very long time: 25-30 hrs of solid working time (I had to do a lot of background reading to catch myself up - big stat review and calculus for question 2)


\end{document}
