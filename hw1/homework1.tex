\documentclass[submit]{harvardml}

\course{CS181-S21}
\assignment{Assignment \#1}
\duedate{7:59pm ET, February 4, 2021} 

\usepackage[OT1]{fontenc}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{fullpage}
\usepackage{soul}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{common}
\usepackage{framed}

\usepackage[mmddyyyy,hhmmss]{datetime}

\definecolor{verbgray}{gray}{0.9}

\lstnewenvironment{csv}{
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  columns=fullflexible}}{}
 

\begin{document}
\begin{center}
{\Large Homework 1: Regression}\\
\end{center}

\subsection*{Introduction}
This homework is on different forms of linear regression and focuses
on loss functions, optimizers, and regularization. Linear regression
will be one of the few models that we see that has an analytical
solution.  These problems focus on deriving these solutions and
exploring their properties.

If you find that you are having trouble with the first couple
problems, we recommend going over the fundamentals of linear algebra
and matrix calculus (see links on website).  The relevant parts of the
\href{https://github.com/harvard-ml-courses/cs181-textbook/blob/master/Textbook.pdf}{cs181-textbook notes are Sections 2.1 - 2.7}.  We strongly recommend
reading the textbook before beginning the homework.

    We also encourage you to first read the \href{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf}{Bishop textbook}, particularly:
Section 2.3 (Properties of Gaussian Distributions), Section 3.1
(Linear Basis Regression), and Section 3.3 (Bayesian Linear
Regression). (Note that our notation is slightly different but the
underlying mathematics remains the same!).

\textbf{Please type your solutions after the corresponding problems using this
\LaTeX\ template, and start each problem on a new page.} You may find
the following introductory resources on \LaTeX\ useful: 
\href{http://www.mjdenny.com/workshops/LaTeX_Intro.pdf}{\LaTeX\ Basics} 
and \href{https://www.overleaf.com/learn/latex/Free_online_introduction_to_LaTeX_(part_1)}{\LaTeX\ tutorial with exercises in Overleaf}

Homeworks will be submitted through Gradescope. You will be added to
the course Gradescope once you join the course Canvas page. If you
haven't received an invitation, contact the course staff through Ed.

\textbf{Please submit the writeup PDF to the Gradescope assignment
  `HW1'.} Remember to assign pages for each question.

\textbf{Please submit your \LaTeX file and code files to the
  Gradescope assignment `HW1 - Supplemental'.} Your files should be
named in the same way as we provide them in the repository,
e.g. \texttt{T1\_P1.py}, etc.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{problem}[Optimizing a Kernel, 15pts]

Kernel-based regression techniques are similar to nearest-neighbor
regressors: rather than fit a parametric model, they predict values
for new data points by interpolating values from existing points in
the training set.  In this problem, we will consider a kernel-based
regressor of the form:
\begin{equation*}
  f(x^*) = \frac{ \sum_{n} K(x_n,x^*) y_n  }{ \sum_{n} K(x_n,x^*) } 
\end{equation*}
where $(x_n,y_n)$ are the training data points, and $K(x,x')$ is a
kernel function that defines the similarity between two inputs $x$ and
$x'$. Assume that each $x_i$ is represented as a column vector, i.e. a
$D$ by 1 vector where $D$ is the number of features for each data
point. A popular choice of kernel is a function that decays as the
distance between the two points increases, such as
\begin{equation*}
  K(x,x') = \exp(-||x-x'||^2_2) = \exp(-(x-x')^T (x-x') ) 
\end{equation*} 
However, the squared Euclidean distance $||x-x'||^2_2$ may not always
be the right choice.  In this problem, we will consider optimizing
over squared Mahalanobis distances
\begin{equation*}
  K(x,x') = \exp(-(x-x')^T W (x-x') )
  \label{eqn:distance}
\end{equation*} 
where $W$ is a symmetric $D$ by $D$ matrix.  Intuitively, introducing
the weight matrix $W$ allows for different dimensions to matter
differently when defining similarity.

\begin{enumerate}

\item Let $\{(x_n,y_n)\}_{n=1}^N$ be our training data set.  Suppose
  we are interested in minimizing the residual sum of squares.  Write down this
  loss over the training data $\mcL(W)$ as a function of $W$.

  Important: When computing the prediction $f(x_i)$ for a point $x_i$
  in the training set, carefully consider for which points $x'$ you should be including
  the term $K(x_i,x')$ in the sum.
  
\item In the following, let us assume that $D = 2$.  That means that
  $W$ has three parameters: $W_{11}$, $W_{22}$, and $W_{12} = W_{21}$.
  Expand the formula for the loss function to be a function of these
  three parameters.
  
 \item Derive the gradients of the loss function with respect to each of the parameters of $W$ for the $D=2$ case. (This will look a bit messy!)



\end{enumerate}
\end{problem}

\newpage

\begin{framed}
\noindent\textbf{Problem 1} (cont.)\\
\begin{enumerate}
\setcounter{enumi}{3}
\item Consider the following data set:
\begin{csv}
x1 , x2 , y 
  0 , 0 , 0
  0 , .5 , 0
  0 , 1 , 0 
  .5 , 0 , .5
  .5 , .5 , .5
  .5 , 1 , .5
  1 , 0 , 1
  1 , .5 , 1
  1 , 1 , 1 
\end{csv}
And the following kernels:
\begin{equation*} 
W_1 = \alpha \begin{bmatrix}
  1 & 0 \\
  0 & 1 
\end{bmatrix}
\qquad
W_2 = \alpha \begin{bmatrix}
  0.1 & 0 \\
  0 & 1 
\end{bmatrix}
\qquad
W_3 = \alpha \begin{bmatrix}
  1 & 0 \\
  0 & 0.1 
\end{bmatrix}
\end{equation*} 
with $\alpha = 10$. Write some Python code to compute the loss with
respect to each kernel for the dataset provided above. Which kernel
does best?  Why?  How does the choice of $\alpha$ affect the loss? 

For this problem, you can use our staff \textbf{script to compare your code to a set of staff-written test cases.} This requires, however, that you use the structure of the starter code provided in \texttt{T1\_P1.py}. More specific instructions can be found at the top of the file \texttt{T1\_P1\_Testcases.py}. You may run the test cases in the command-line using \texttt{python T1\_P1\_TestCases.py}.
\textbf{Note that our set of test cases is not comprehensive: just because you pass does not mean your solution is correct! We strongly encourage you to write your own test cases and read more about ours in the comments of the Python script.}

\item Bonus:  Code up a gradient descent to
  optimize the kernel for the data set above.  Start your gradient
  descent from $W_1$.  Report on what you find.\\
  Gradient descent is discussed in Section 3.4 of the cs181-textbook notes and Section 5.2.4 of Bishop, and will be covered later in the course! 

\end{enumerate}
  
\end{framed}  


\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Solution 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Solution 1}
\begin{enumerate}
    \item \textbf{Write down the loss $\mcL(W)$ over the training data as a function of $W$:}\\
    
    Firstly the L2 loss function is defined as follows:
    
    $$
        \mcL(W) = \sum^{n}{(y_n - \hat{y}_n)^2}
    $$
    
    Next we can take our definition of kernelized regression with Mahalanobonis distances used for the kernel function:
    $$
        \hat{y}(a)= \frac{\sum^n exp(-(a -x_n)^T W(a - x_n)) y_n}
                        {\sum^n exp(-(a -x_n)^T W(a - x_n)}
    $$
    
    Now we can substitute the kernel into the L2 function, excluding the case where $b=a$:
    $$
        \mcL(W) =  \sum^{n}_{a = 1}{\left[
            \left(y_a - \left( \frac  {\sum^n_{\{b\in \{1, ...,n\} | b \neq a\}} exp(-(x_a -x_b)^T W(x_a - x_b)) y_n}
                            {\sum^n_{\{b\in \{1, ...,n\} | b \neq a\}} exp(-(x_a -x_b)^T W(x_a - x_b))}\right)
            \right)^2
        \right]}
    $$
    
    \item \textbf{Expand the loss function to include $w_{11}$, $w_{12}$, $w_{22}$:}\\
    
    In order to simplify the above equation we can introduce a shorthand for $(x_a - x_b)$:
    $$ Let z = x_a - x_b $$
    As we are assuming $D = 2$, we can  assign the following:\\
    $z$ will be a 2 component column vector: 
    $z = 
    \begin{bmatrix}
        z_i \\ 
        z_j 
    \end{bmatrix}$ \\
    
    Likewise, $W = 
    \begin{bmatrix}
        w_{11} & w_{12} \\
        w_{21} & w_{22}
    \end{bmatrix}$\\
    
    Now we can substitute the $z$ values into our equation:
    $$
        \mcL(W) = \sum^{n}_{a = 1}{\left[
            \left(y_a - \left( \frac  {\sum^n_{\{b\in \{1, ...,n\} | b \neq a\}} exp(-(z)^T W(z)) y_n}
                            {\sum^n_{\{b\in \{1, ...,n\} | b \neq a\}} exp(-(z)^T W(z)})\right)
            \right)^2
        \right]}
    $$
    
    We can simplify by multiplying out $z^{T}Wz$ prior to substitution:
    
    $$
        \begin{bmatrix}
            z_i & z_j\\
        \end{bmatrix}
        \begin{bmatrix}
            w_{11} & w_{12} \\
            w_{21} & w_{22}
        \end{bmatrix}
        \begin{bmatrix}
            z_i \\
            z_j
        \end{bmatrix}
        = w_{11}z_{i}^{2} + 2w_{12}z_{i}z_{j} + w_{22}z_j^2 
    $$
    This can now be substituted into the loss function expression, as $w_{12} = w_{21}$ they have all been set to $w_{12}$ for simplicity.
    $$
        \mcL(W) = \sum^{n}_{a = 1}{\left[
            \left(y_a - \left( \frac  {\sum^n_{\{b\in \{1, ...,n\} | b \neq a\}} exp(-(w_{11}z_{i}^{2} + 2w_{12}z_{i}z_{j} + w_{22}z_j^2)) y_n}
                            {\sum^n_{\{b\in \{1, ...,n\} | b \neq a\}} exp(-(w_{11}z_{i}^{2} + 2w_{12}z_{i}z_{j} + w_{22}z_j^2))}\right)
            \right)^2
        \right]}
    $$
    
    \item \textbf{Derive the gradients of the Loss function.}\\
    \item See T1\_P1.py file for results.
\end{enumerate}
\newpage 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{problem}[Kernels and kNN, 10pts]

Now, let us compare the kernel-based approach to an approach based on
nearest-neighbors.  Recall that kNN uses a predictor of the form

  \begin{equation*}
    f(x^*) = \frac{1}{k} \sum_n y_n \mathbb{I}(x_n \texttt{ is one of k-closest to } x^*)
  \end{equation*}

\noindent where $\mathbb{I}$ is an indicator variable. For this problem, you will use the same kernels as Problem 1, and dataset \verb|data/p2.csv|. 

For this problem, you can use our staff \textbf{script to compare your code to a set of staff-written test cases.} This requires, however, that you use the structure of the starter code provided in \texttt{T1\_P2.py}. More specific instructions can be found at the top of the file \texttt{T1\_P2\_Testcases.py}. You may run the test cases in the command-line using \texttt{python T1\_P2\_TestCases.py}.
\textbf{Note that our set of test cases is not comprehensive: just because you pass does not mean your solution is correct! We strongly encourage you to write your own test cases and read more about ours in the comments of the Python script.}

\vspace{0.5cm}
\noindent\emph{Make sure to include all required plots in your PDF.}


\begin{enumerate}

\item We will be making 6 plots comparing kernel-based and nearest
  neighbor-based predictors, all using the Mahalanobis distance
  corresponding to $W_1$ from Problem 1. In each plot, you will plot
  the predicted value of $y$, given $x_1$ (horizontal axis) and $x_2$
  (vertical axis), as the color of each point (grayscale
  between $0$ and $1$). Include the $x_1$ and $x_2$ axes, with tick marks spaced every 0.1 units
  for $x_1=0$ to $x_1=1$ and $x_2=0$ to $x_2=1$.
  
  For the first three plots, use the kernel-based predictor varying
  $\alpha = \{0.1,3,10\}$.  For the next three plots, use the kNN
  predictor with $\alpha = 1$, $k=\{1,5,N-1\}$, where $N$ is the size
  of the data set.

  Print the total least squares loss on the training set for each of
  the 6 plots.
  
  You may choose to use some starter Python code to create your plots
  provided in \verb|T1_P2.py|.  Please \textbf{write your own
    implementation of kNN} for full credit.  Do not use external
  libraries to find nearest neighbors.
  
\item Do any of the kernel-based regression plots look like the 1NN?
  The $(N-1)$NN?  Why or why not?

\item Suppose we are given some $W$ for a Mahalanobis distance or
  kernel function.  Then, in general, there exist values of $k$ for which
  kernel-based regression and kNN disagree (i.e., make different predictions)
  on at least one input - for all choices of $\alpha$. Explain why by means of
  an example (i.e., show that for some value $k$ of your choosing,
  no value of $\alpha$ will produce two classifiers that are the same).
    
\item Why did we not vary $\alpha$ for the kNN approach?    

\end{enumerate}

\end{problem}


\newpage 
\textbf{Solution 2}
\begin{enumerate}
    \item The predicted value of Y given X:\\
    \begin{enumerate}
        \item Using Kernel method with an alpha of 0.1, L2 loss = 1.8399712540879825 \\
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw1/T1P2_plots/plot1.png}
            \centering
        \end{figure}
        
        \item Using Kernel method with an alpha of 3, L2 loss = 0.6200161545448001\\
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw1/T1P2_plots/plot2.png}
            \centering
        \end{figure}
        
        \item Using Kernel method with an alpha of 10, L2 loss = 0.39001293585550434 \\
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw1/T1P2_plots/plot3.png}
            \centering
        \end{figure}
        
        \item Using Knn method with k=1, L2 loss = 0.8383999999999999 \\
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw1/T1P2_plots/plot4.png}
            \centering
        \end{figure}
        
        \item Using Knn method with k=5, L2 loss = 0.4692999999999999 \\
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw1/T1P2_plots/plot5.png}
            \centering
        \end{figure}
        
        \item Using Knn method with k=12, L2 loss = 1.9225736111111114 \\
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw1/T1P2_plots/plot6.png}
            \centering
        \end{figure}
        
    \end{enumerate}
    \item 
        In the 1NN case we are only looking at the closest points to our guess value in our dataset, when we apply a very large $\alpha$, it will greatly increase the distance of far away points - thereby reducing their impact on our prediction. Having a small impact on our prediction and completely ignoring points will have a similar effect. We can see evidence for this is the case where $\alpha = 10$ and $k=1$ as the plots look similar.\\
        In the same sense a very small $\alpha$ will increase the weighting of far away points, which is similar to setting $k=(N-1)$. With a large $k$ we are including all of the points in our prediction, both are acting as taking a mean value across the dataset shown by the similarity between the $\alpha = 0.1$ and the $k=1$ plots.
    
    \item In the case where $k=1$ there will be a disagreement between kernel based and kNN. In the previous question I showed that the $k=1$ will be similar to a large alpha - while they are similar their will always be a difference between the predicted values for some input - this is because to some degree all of the other points must be taken into consideration thereby altering the prediction to a degree. The larger $\alpha$ gets, the closer the prediction will be to $k=1$ but the prediction will only be the same when $\alpha = \infty$ which would break the classifier - therefore there exists a $k$ for which the two disagree.
    
    \item Alpha increases the distance of far away points (i.e. for a given distance away the kernel will discriminate it further). When using kNN we can only select the nearest k points, so we have no need to further reduce the impact of far points using alpha.

\end{enumerate}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}[Deriving Linear Regression, 10pts]

  In class, we noted that the solution for the least squares linear
  regressions ``looked'' like a ratio of covariance and variance
  terms.  In this problem, we will make that connection more explicit.

  Let us assume that our data are tuples of scalars $(x,y)$ that come from
  some distribution $p(x,y)$.  We will consider the process of fitting
  these data with the best linear model possible, that is a linear
  model of the form $\hat{y} = wx$ that minimizes the expected squared
  loss $E_{x,y}[ ( y - \hat{y} )^2 ]$.\\

\noindent \emph{Notes:} The notation $E_{x, y}$ indicates an
expectation taken over the joint distribution $p(x,y)$.  Since $x$ and
$y$ are scalars, $w$ is also a scalar.
  
  \begin{enumerate}

  \item Derive an expression for the optimal $w$, that is, the $w$
    that minimizes the expected squared loss above.  You should leave
    your answer in terms of moments of the data, e.g. terms like
    $E_x[x]$, $E_x[x^2]$, $E_y[y]$, $E_y[y^2]$, $E_{x,y}[xy]$ etc.

\item Provide unbiased and consistent formulas to estimate $E_{x, y}[yx]$
 and $E_x[x^2]$ given observed data $\{(x_n,y_n)\}_{n=1}^N$.

\item In general, moment terms like $E_{x, y}[yx]$, $E_{x, y}[x^2]$,
  etc. can easily be estimated from the data (like you did above).  If
  you substitute in these empirical moments, how does your expression
  for the optimal $w^*$ in this problem compare with the optimal $w^*$
  that we derived in class/Section 2.6 of the cs181-textbook?

\item As discussed in lecture, many common probabilistic linear regression models assume that variables x and y are jointly Gaussian.  Did any of your above derivations rely on the assumption that x and y are jointly Gaussian?  Why or why not?
    
\end{enumerate}

\end{problem}

\newpage
\textbf{Solution 3}
\begin{enumerate}
    \item Let the cost $\mcL(w) = C$.\\
    We can then write the total loss as: 
    $$
    \sum^n = (y = wx)^2
    $$
    
    To find the optimal value of $w$ we can take the derivative of the cost $C$ with respect to $w$:
    $$
        \frac{\delta C}{\delta w} \sum (y - wx)^2
        = \sum -2x(y - wx)  
    $$
    Now we can set the gradient to 0 and solve for $w$, dropping the constants:
    $$
        0 = \sum -2x(y - wx) = \sum (yx - wx^2)  \\
        = \sum (yx) - \sum x^2 
    $$
    Therefore, we can write this in moments as:
    $$
        w =\frac{E [xy]}{E [x^2]}
    $$
    
    \item We can proved unbiased formulas for the above terms using the definition of expectation: \\
    
    $$
        E[x] = \frac{\sum x}{N}
    $$
    
    Similarly the formula for $E[xy]$ is:
    $$
        E[xy] = \frac{\sum xy}{N}
    $$
    
    \item In the textbook the following definition is given for calculating $w*$:
    $$
        0 =  \sum^N_{n=1} y_n x^T_n - w^T\sum^N_{n=1} (x_n x^T_n)
    $$
    As we are working purely with scalars in this example, we can take advantage of the fact that $x_n x_n^T = x_n^2$:
    $$
        0 = \sum^N_{n=1} y_n x_n - w^T\sum^N_{n=1} (x_n^2)
    $$
    
    Rearranging we can get:
    $$
        w = \frac{\sum^N_{n=1} y_n x_n}{w^T\sum^N_{n=1} (x_n^2)}
    $$
    
    Which when substituting in moments:
    $$
        w =\frac{E [xy]}{E [x^2]}
    $$
    
    \item While the above derivations themselves do not rely on the fact that $x$ and $y$ are jointly Guassian, derivation 2.6.2 in the textbook shows that a solution can also be found using \textit{maximum likelyhood estimation}. \\
    Here we assume x and y are Gaussian and use the density function of a univariate Gaussian. However, we still end up at the Moore-Pensrose pseudo inverse!
    $$
        \frac{\delta \ln{ p(Y|X,w,\Beta) } }{\delta w} = (X^TX)^{-1}X^TY
    $$
    
    To answer the question, the regression models do not assume any joint Guassian properties.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}[Modeling Changes in Republicans and Sunspots, 15pts]
  
 The objective of this problem is to learn about linear regression
 with basis functions by modeling the number of Republicans in the
 Senate. The file \verb|data/year-sunspots-republicans.csv| contains the
 data you will use for this problem.  It has three columns.  The first
 one is an integer that indicates the year.  The second is the number
 of Sunspots observed in that year.  The third is the number of Republicans in the Senate for that year.
 The data file looks like this:
 \begin{csv}
Year,Sunspot_Count,Republican_Count
1960,112.3,36
1962,37.6,34
1964,10.2,32
1966,47.0,36
\end{csv}

You can see scatterplots of the data in the figures below.  The horizontal axis is the Year, and the vertical axis is the Number of Republicans and the Number of Sunspots, respectively.

\begin{center}
\includegraphics[width=.5\textwidth]{data/year-republicans}
\end{center}

\begin{center}
\includegraphics[width=.5\textwidth]{data/year-sunspots}
\end{center}

(Data Source: \url{http://www.realclimate.org/data/senators_sunspots.txt})\\
\vspace{-5mm}


\vspace{0.5cm}
\noindent\emph{Make sure to include all required plots in your PDF.}

\begin{enumerate}

\item In this problem you will implement ordinary least squares regression using 4 different basis functions for
\textbf{Year (x-axis)} v. \textbf{Number of Republicans in the Senate (y-axis)}. Some starter Python code
that implements simple linear regression is provided in \verb|T1_P4.py|.

First, plot the data and regression lines for each of the following sets of basis functions, and include
the generated plot as an image in your submission PDF. You will therefore make 4 total plots:
\begin{enumerate}
	\item[(a)] $\phi_j(x) = x^j$ for $j=1, \ldots, 5$\\
    ie, use basis $y = a_1 x^1 + a_2 x^2 + a_3 x^3 + a_4 x^4 + a_5 x^5$ for some constants $\{a_1, ..., a_5\}$. 
    \item[(b)] $\phi_j(x) = \exp{\frac{-(x-\mu_j)^2}{25}}$ for $\mu_j=1960, 1965, 1970, 1975, \ldots 2010$
	\item[(c)] $\phi_j(x) = \cos(x / j)$ for $j=1, \ldots, 5$
	\item[(d)] $\phi_j(x) = \cos(x / j)$ for $j=1, \ldots, 25$
\end{enumerate}
\vspace{-2mm}
{\footnotesize * Note: Be sure to add a bias term for each of the basis functions above.}

Second, for each plot include the residual sum of squares error. Submit the generated plot and residual sum-of-squares error for each basis in your LaTeX write-up.
\end{enumerate}

\end{problem}

\begin{framed}
\noindent\textbf{Problem 4} (cont.)\\
\begin{enumerate}
\setcounter{enumi}{1}
\item Repeat the same exact process as above but for \textbf{Number of Sunspots (x-axis)} v. \textbf{Number of Republicans in the Senate (y-axis)}. 
Now, however, only use data from before 1985, and only use basis functions (a), (c), and (d) -- ignore basis (b). You will therefore make 3 total plots. For each plot make sure to also include the residual sum of squares error.

Which of the three bases (a, c, d) provided the "best" fit? \textbf{Choose one}, and keep in mind the generalizability of the model. 

Given the quality of this fit, do you believe that the number of sunspots controls the number of Republicans in the senate (Yes or No)?
\end{enumerate}
\end{framed}

\newpage

\textbf{Solution 4}
\begin{enumerate}
    \item \textbf{Plot each of the basis functions applied to the Year V. Number of Republicans in Senate.}
    \begin{enumerate}
    	\item[(a)] $\phi_j(x) = x^j$ for $j=1, \ldots, 5$:\\
    	L2 Loss:  394.9803839890865 \\
    	\begin{figure}[H]
            \includegraphics[width=8cm]{hw1/T1P4_plots/plot0.png}
            \centering
        \end{figure}
        \item[(b)] $\phi_j(x) = \exp{\frac{-(x-\mu_j)^2}{25}}$ for $\mu_j=1960, 1965, 1970, 1975, \ldots 2010$: \\
        L2 Loss:  54.2730966167196:\\
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw1/T1P4_plots/plot1.png}
            \centering
        \end{figure}
    	\item[(c)] $\phi_j(x) = \cos(x / j)$ for $j=1, \ldots, 5$: \\
    	L2 Loss:  1082.8088559867185 \\
    	\begin{figure}[H]
            \includegraphics[width=8cm]{hw1/T1P4_plots/plot2.png}
            \centering
        \end{figure}
    	\item[(d)] $\phi_j(x) = \cos(x / j)$ for $j=1, \ldots, 25$: \\
    	L2 Loss:  39.001226916562295 \\
    	\begin{figure}[H]
            \includegraphics[width=8cm]{hw1/T1P4_plots/plot3.png}
            \centering
        \end{figure}
    \end{enumerate}
    
    \item \textbf{Plot each of the basis functions applied to the Number of Sunspots V. Number of Republicans in Senate.}
    \begin{enumerate}
    	\item[(a)] $\phi_j(x) = x^j$ for $j=1, \ldots, 5$:\\
    	L2 Loss:  1156.987832119872 \\
    	\begin{figure}[H]
            \includegraphics[width=8cm]{hw1/T1P4_plots/plot4.png}
            \centering
        \end{figure}
        \item[(b)] $\phi_j(x) = \exp{\frac{-(x-\mu_j)^2}{25}}$ for $\mu_j=1960, 1965, 1970, 1975, \ldots 2010$: \\
        L2 Loss:  990.565015704863:\\
        \begin{figure}[H]
            \includegraphics[width=8cm]{hw1/T1P4_plots/plot5.png}
            \centering
        \end{figure}
    	\item[(d)] $\phi_j(x) = \cos(x / j)$ for $j=1, \ldots, 25$: \\
    	L2 Loss:  604.716476582372 \\
    	\begin{figure}[H]
            \includegraphics[width=8cm]{hw1/T1P4_plots/plot6.png}
            \centering
        \end{figure}
    \end{enumerate}
    
\end{enumerate}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Name and Calibration
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Name: Phil Labrum}

\subsection*{Collaborators and Resources}
Natalie Margulies, Tommy Maldonado

\subsection*{Calibration}
Approximately how long did this homework take you to complete (in hours)? 

\end{document}



